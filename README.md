# Trexquant Hangman Challenge

**Author:** Atul Kumar Pandey  
**Approach:** BiLSTM + Regex Fallback

---

## ğŸ§  Overview

This project solves Hangman using a hybrid of deep learning and rule-based methods. A BiLSTM model predicts missing letters from masked words, and regex-based fallbacks handle uncertain cases.

---

## ğŸ§¾ Dataset

- Words augmented by masking random/consonant letters
- Input: 35-length padded sequences with `{a-z} â†’ 1â€“26`, `_ â†’ 27`, pad â†’ 0
- Output: 26D binary vector of present letters
- Vowel priors stored for improved fallback guessing

---

## ğŸ§± Model

- Embedding â†’ BiLSTM (128) â†’ BiLSTM (64) â†’ Dense (48) â†’ Output (26 sigmoid)
- Trained with `binary_crossentropy` & SGD
- Saved as `bi_lstm.weights.h5`

---

## ğŸ” Fallback

- Uses regex to match word patterns in dictionary
- Chooses letter with highest occurrence in candidates
- Vowel priors assist if model is uncertain

---

## ğŸ¯ Results

| Threshold | Accuracy (%) |
|-----------|---------------|
| 0.55      | 48            |
| 0.60      | 52            |
| 0.65      | 58            |

A final accuracy of 61% was obtained on test runs across
1000 different words.


