{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84026610-7b08-4b42-9fc7-2564b16f60ce",
   "metadata": {},
   "source": [
    "# Creation of a dataset.\n",
    "\n",
    "We create a dataset from 220k given words to mirror hangman game. \n",
    "We replace all occurences \n",
    "of a few letters by '_' and set that letter as a target letter.\n",
    "Then we save X and y as .npy file and will be using it for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05b4ee-02b3-43f0-89cc-941bb9ec68e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18e337-e39a-43c8-a628-43d1dc475b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 35\n",
    "CHAR_MAP = {c: i + 1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "CHAR_MAP['_'] = 27\n",
    "PAD_VAL = 0\n",
    "VOWELS = set(\"aeiou\")\n",
    "\n",
    "def encode_masked_word(word):\n",
    "    encoded = [CHAR_MAP[c] for c in word]\n",
    "    return [PAD_VAL] * (MAX_LEN - len(encoded)) + encoded\n",
    "\n",
    "def encode_target(word):\n",
    "    vec = [0] * 26\n",
    "    for c in set(word):\n",
    "        if c in CHAR_MAP and c != '_':\n",
    "            vec[CHAR_MAP[c] - 1] = 1\n",
    "    return vec\n",
    "\n",
    "def load_words(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        words = [line.strip().lower() for line in f if line.strip()]\n",
    "    return [w for w in words if 3 < len(w) <= 29 and len(set(w)) > 2 and VOWELS.intersection(set(w))]\n",
    "\n",
    "def permute_all(word, vowel_mode=False):\n",
    "    unique_letters = list(set(word))\n",
    "    max_mask = len(unique_letters) - (1 if vowel_mode else 2)\n",
    "    all_perm = set()\n",
    "    for i in range(max_mask):\n",
    "        random_letters = random.sample(unique_letters, i + 1)\n",
    "        new_word = word\n",
    "        for l in random_letters:\n",
    "            new_word = new_word.replace(l, '_')\n",
    "        all_perm.add(new_word)\n",
    "    return list(all_perm)\n",
    "\n",
    "def permute_consonants(word):\n",
    "    vowels_only = \"\".join([c if c in VOWELS else \"_\" for c in word])\n",
    "    vowel_indices = [i for i, c in enumerate(vowels_only) if c != \"_\"]\n",
    "    abridged = vowels_only.replace(\"_\", \"\")\n",
    "    perms = permute_all(abridged, vowel_mode=True)\n",
    "    \n",
    "    results = []\n",
    "    for p in perms:\n",
    "        temp = ['_'] * len(word)\n",
    "        for i, c in enumerate(p):\n",
    "            temp[vowel_indices[i]] = c\n",
    "        results.append(\"\".join(temp))\n",
    "    return results\n",
    "\n",
    "# === Vowel Prior Support ===\n",
    "def get_vowel_prob(df_vowel, vowel):\n",
    "    try:\n",
    "        return df_vowel[0].apply(lambda p: vowel in p).value_counts(normalize=True).loc[True]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_vowel_prior(df_aug):\n",
    "    prior_json = {}\n",
    "    for word_len in range(1, df_aug[1].max() + 1):\n",
    "        df_vowel = df_aug[df_aug[1] == word_len]\n",
    "        probs = [get_vowel_prob(df_vowel, v) for v in ['a', 'e', 'i', 'o', 'u']]\n",
    "        prior_json[word_len] = pd.DataFrame([\n",
    "            pd.Series(['a', 'e', 'i', 'o', 'u']),\n",
    "            pd.Series(probs)\n",
    "        ]).T.sort_values(by=1, ascending=False)\n",
    "    return prior_json\n",
    "\n",
    "def save_vowel_prior(vowel_prior):\n",
    "    pickle.dump(vowel_prior, open(\"prior_probabilities.pkl\", \"wb\"))\n",
    "\n",
    "def generate_dataset(words):\n",
    "    X_data, y_data = [], []\n",
    "    aug_list = []  # For vowel prior calculation\n",
    "\n",
    "    for word in words:\n",
    "        y = encode_target(word)\n",
    "        aug_list.append((word, len(word)))\n",
    "        masked_set = set(permute_all(word) + permute_consonants(word))\n",
    "        for masked in masked_set:\n",
    "            X_data.append(encode_masked_word(masked))\n",
    "            y_data.append(y)\n",
    "    df_aug = pd.DataFrame(aug_list)\n",
    "    vowel_prior = get_vowel_prior(df_aug)\n",
    "    save_vowel_prior(vowel_prior)\n",
    "    print(\"Saved vowel priors to prior_probabilities.pkl\")\n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "def save_dataset(X, y, prefix='hangman'):\n",
    "    np.save(f\"{prefix}_X.npy\", X)\n",
    "    np.save(f\"{prefix}_y.npy\", y)\n",
    "    print(f\"Saved dataset: {prefix}_X.npy shape={X.shape}, {prefix}_y.npy shape={y.shape}\")\n",
    "\n",
    "# === Execute ===\n",
    "words = load_words(\"words_250000_train.txt\")\n",
    "X, y = generate_dataset(words)\n",
    "save_dataset(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846943f-4f82-4c21-abc3-7723841027f5",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We create an LSTM model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53508b1-5fd7-4719-91ed-837b33be5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=64, output_dim=64, input_length=35,\n",
    "                                  embeddings_constraint=tf.keras.constraints.MaxNorm(1)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False, dropout=0.2)),\n",
    "        tf.keras.layers.Dense(48, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.20),\n",
    "        tf.keras.layers.Dense(26, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_model(x_path=\"hangman_X.npy\", y_path=\"hangman_y.npy\", epochs=8, batch_size=128, val_split=0.02):\n",
    "    # Load data\n",
    "    X = 0\n",
    "    y = 0\n",
    "    X = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    # Split into train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=42)\n",
    "\n",
    "    model = create_model()\n",
    "    model_path = \"bi_lstm.weights.h5\"\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing model weights...\")\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model.save_weights(model_path)\n",
    "    print(\"Training complete. Model weights saved!\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457ab9a-b07b-4ae5-a445-e144427af0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c8cac-1fb6-45e5-960e-ebbc7378dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth on {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Could not set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73add5-764c-4a0e-ba94-c25241a7e4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766160d-3be9-48b5-aa29-51779bb169ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818adddb-3f9e-4bc1-9c5f-4645b6bc7baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3950711-1642-4c40-a1b0-6c7de40aa9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7b7a8-c9b4-4032-9270-ff32f97d8306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412345b2-332b-4247-9f33-0b25cca5f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
